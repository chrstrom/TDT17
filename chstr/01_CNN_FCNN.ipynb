{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description from Frank\n",
    "One task that you can already start working on (since this was part of our first meeting)\n",
    "is a numerical example of one complete forward backward learning cycle through a very simple\n",
    "classification network consisting of a feature extraction/learning part (CNN with conv. and\n",
    "pooling layers) and a classification part (FCNN with at least one fully-connected/dens layer)\n",
    "\n",
    "To get you started on something practical (first task) you should look under “Files”\n",
    "here: 10_numExampleOrVideo(channel) > 2022 > 2022-P0> 2022-P0-T1_FCNNs & CNNs -F&B pass”\n",
    "Here you will find the file“Guide_Part2.pdf” that will give you more information about the task.\n",
    "There is also a folder “00_Examples_Given” that provides you with some worked out examples.\n",
    "\n",
    "\n",
    "# TODO:\n",
    "For FFN, see https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "\n",
    "For CNN: https://towardsdatascience.com/lets-code-convolutional-neural-network-in-plain-numpy-ce48e732f5d5\n",
    "\n",
    "Base the work off the T3.1 and lecture slides. There does not need to be any \"point\" behind the network per say, since the\n",
    "\"only important\" part here is the math.\n",
    "\n",
    "1. Create the definition of the network as a parameter list\n",
    "2. DONE Create helper functions for activations\n",
    "3. DONE Create matrix visualizer.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem formulation\n",
    "\n",
    "The parameter table for the network should go here, as well as a short introduction to what this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "class Matrix2D():\n",
    "\n",
    "    def __init__(self, array: np.ndarray):\n",
    "        assert len(array.shape) == 2, \"Input array must be 2D!\"\n",
    "\n",
    "        self.value: np.ndarray = array\n",
    "        self.nrow: int = array.shape[0]\n",
    "        self.ncol: int = array.shape[1] \n",
    "\n",
    "    def draw(self, title: str = \"\") -> None:\n",
    "        \"\"\"Display a visualization of the matrix values\n",
    "\n",
    "        See: https://stackoverflow.com/questions/40887753/display-matrix-values-and-colormap\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.matshow(self.value, cmap=plt.cm.coolwarm)\n",
    "\n",
    "        for y, x in np.ndindex(self.value.shape):\n",
    "            ax.text(y, x, str(self.value[x, y]), va='center', ha='center')\n",
    "        ax.set_title(title)\n",
    "\n",
    "\n",
    "    # TODO: Fix padding\n",
    "    def convolve2D(self, kernel: Matrix2D, bias: float = 0, stride: int = 1) -> Matrix2D:\n",
    "        \"\"\"Convolve the 2D matrix with a 2D kernel plus bias\n",
    "\n",
    "        See http://www.songho.ca/dsp/convolution/convolution2d_example.html for the math\n",
    "        \"\"\"\n",
    "        assert stride != 0, \"Stride cannot be zero!\"\n",
    "\n",
    "        output_width: int = (self.nrow - kernel.nrow) // stride + 1\n",
    "        output_height: int = (self.ncol - kernel.ncol) // stride + 1\n",
    "\n",
    "        output_image = np.zeros((output_height, output_width))\n",
    "\n",
    "        for col, row in product(range(0, output_width, stride), range(0, output_height, stride)):\n",
    "            output_image[row, col] = \\\n",
    "                -1 * self.value[row:row+kernel.ncol, col:col+kernel.nrow].flatten() \\\n",
    "                @ kernel.value.flatten() \\\n",
    "                + bias \n",
    "\n",
    "        return Matrix2D(output_image)\n",
    "\n",
    "\n",
    "# TODO: Make compatible with arrays and matrices?\n",
    "# TODO: Alternatively remove class and just pass bool\n",
    "class Activation():\n",
    "    def __init__(self, backward: bool = False):\n",
    "        self.backward = backward\n",
    "\n",
    "    def relu(self, value: float):\n",
    "        \"\"\"ReLU activation\"\"\"\n",
    "        act = value > 0\n",
    "\n",
    "        if self.backward:\n",
    "            return act * 1\n",
    "\n",
    "        return act * value\n",
    "\n",
    "    def sigmoid(self, value: float):\n",
    "        \"\"\"Sigmoid activation\"\"\"\n",
    "        act = np.exp(value) / (1 + np.exp(value))\n",
    "\n",
    "        if self.backward:\n",
    "            return act * (1 - act)\n",
    "\n",
    "        return act\n",
    "\n",
    "act = Activation()\n",
    "\n",
    "x = 10.0\n",
    "\n",
    "print(act.relu(x))\n",
    "act.backward = True\n",
    "print(act.relu(x))\n",
    "\n",
    "\n",
    "# Sanity check using the example from http://www.songho.ca/dsp/convolution/convolution2d_example.html\n",
    "\n",
    "data = np.zeros((5, 5))\n",
    "data[1:4, 1:4] = np.arange(1, 10).reshape((3, 3))\n",
    "X = Matrix2D(data)\n",
    "#X.draw()\n",
    "K = Matrix2D(np.array(([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])))\n",
    "\n",
    "O = X.convolve2D(K)\n",
    "#O.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do forward pass here'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Do forward pass here\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Float:\n",
    "    \"\"\"A wrapper class for numbers in the network that adds additional\n",
    "    functionality like storing the gradient to allow for easy backward\n",
    "    propagation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value, children=()):\n",
    "        self.value = value\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self.backward = lambda: None\n",
    "        self.children = set(children)\n",
    "\n",
    "    def __cast_to_self(self, other):\n",
    "        return other if isinstance(other, Float) else Float(other)\n",
    "\n",
    "    def __add__(self, other) -> Float:\n",
    "        other = self.__cast_to_self(other)\n",
    "        out = Float(self.value + other.value, (self, other))\n",
    "\n",
    "        def backward() -> None:\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out.backward = backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other) -> Float:\n",
    "        other = self.__cast_to_self(other)\n",
    "        out = Float(self.value * other.value, (self, other))\n",
    "\n",
    "        def backward() -> None:\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        \n",
    "        out.backward = backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other) -> Float:\n",
    "        other = self.__cast_to_self(other)\n",
    "        out = Float(self.value**other, (self,))\n",
    "\n",
    "        def backward() -> None:\n",
    "            self.grad += (other * self.value**(other - 1)) * out.grad\n",
    "\n",
    "        out.backward = backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "        \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Float(data={self.value}, grad={self.grad})\"\n",
    "\n",
    "\n",
    "    # Below is a collection of activation functions\n",
    "\n",
    "    def relu(self) -> Float:\n",
    "        value = max(0, self.value)\n",
    "        out = Float(value, (self,))\n",
    "\n",
    "        def backward() -> None:\n",
    "            self.grad += (value > 0) * out.grad\n",
    "\n",
    "        out.backward = backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self) -> Float: \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def abs(self) -> Float:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def linear(self) -> Float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "    # The main workhorse of the backwards pass\n",
    "\n",
    "    def backward_pass(self):\n",
    "        \"\"\"The main algorithm for a backwards pass. Works by topologically\n",
    "        sorting all connected nodes then iteratively and reversely calling\n",
    "        backwards on the sorted list.\n",
    "        \"\"\"\n",
    "        nodes_sorted = []\n",
    "        nodes_visited = []\n",
    "\n",
    "        def topological_sort(node) -> None:\n",
    "            if node not in nodes_visited:\n",
    "                nodes_visited.append(node)\n",
    "\n",
    "                for child in node.children:\n",
    "                    topological_sort(child)\n",
    "\n",
    "                nodes_sorted.append(node)\n",
    "\n",
    "        topological_sort(self)\n",
    "\n",
    "        self.grad = 1 # Need to set this to non-zero, else all grads will be 0\n",
    "        for node in reversed(nodes_sorted):\n",
    "            node.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micrograd inspired FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base():\n",
    "    \n",
    "    def reset(self):\n",
    "        for p in self.p():\n",
    "            p.grad = 0\n",
    "\n",
    "    def p(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Neuron(Base):\n",
    "    \n",
    "    def __init__(self, n_inputs, f=None):\n",
    "        \"\"\"Create a neuron with n inputs.\n",
    "        \n",
    "        Optionally specify an activation function.\n",
    "        If no activation function is set, a default one\n",
    "        will be set. Right now that is f(x) = x\n",
    "        \"\"\"\n",
    "        self.w = [Float(np.random.uniform(-1, 1)) for _ in range(n_inputs)]\n",
    "        self.b = Float(np.random.uniform())\n",
    "        \n",
    "        if f is None:\n",
    "            self.f = lambda x : x # Linear\n",
    "            #self.f = lambda x : max(x, 0) #ReLU\n",
    "        else:\n",
    "            self.f = f\n",
    "    \n",
    "    def z(self, x):\n",
    "        \"\"\"Calculate the pre-activation neuron output\"\"\"\n",
    "        return x@self.w.T + self.b\n",
    "    \n",
    "    def a(self, x):\n",
    "        \"\"\"Calculate the post-activation neuron output\"\"\"\n",
    "        return self.f(self.z(x))\n",
    "\n",
    "    def p(self):\n",
    "        \"\"\"Return the weights of the neuron\"\"\"\n",
    "        return [self.b] + self.w\n",
    "\n",
    "    \n",
    "# Test\n",
    "#N = 3\n",
    "#n = Neuron(N)\n",
    "#x = np.random.rand(N)\n",
    "#print(n.a(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Base):\n",
    "    \n",
    "    def __init__(self, n_neurons, n_inputs):\n",
    "        \"\"\"Create a layer of n_neurons\n",
    "        \n",
    "        The amount of inputs to the neurons will also need to be specified.\n",
    "        \"\"\"\n",
    "        self.neurons = [Neuron(n_inputs) for _ in range(n_neurons)]\n",
    "        \n",
    "    def out(self, x):\n",
    "        \"\"\"Return the vector of outputs of the layer\"\"\"\n",
    "        return [n.out(x) for n in self.neurons]\n",
    "\n",
    "    def p(self):\n",
    "        \"\"\"Return the parameters for all the neurons in the layer\"\"\"\n",
    "        return [p for n in self.neurons for p in n.p()]\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(Base):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \"\"\"Create a fully-connected network of n layers\n",
    "        \"\"\"\n",
    "        sizes = [n_inputs] + n_outputs\n",
    "        self.layers = [Layer(sizes[i], sizes[i+1]) for i in range(len(n_outputs))]\n",
    "        \n",
    "        \n",
    "    def out(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    def p(self):\n",
    "        [p for l in self.layers for p in l.p()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa82987a71ca0339a52998cd22613b0c002bda8349fa32a0cb67ceec3936a2bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
